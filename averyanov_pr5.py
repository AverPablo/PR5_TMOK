# -*- coding: utf-8 -*-
"""Averyanov_PR5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WLr2vMO_0PaMgY8vIw3WHY35uyE-lvJr

1. Выполнить предварительную обработку данных: удалить ненужные столбцы, заполнить пропущенные значения, преобразовать категориальные признаки в числовые.

2. Рассчитать матрицу корреляции между признаками и целевой переменной. Отобрать признаки с наивысшей абсолютной корреляцией.

3. Обучить модель машинного обучения на всем признаковом пространстве и оценить веса признаков. Отобрать признаки с наивысшими весами.

4. Применить метод главных компонент (PCA) для снижения размерности признакового пространства. Отобрать главные компоненты с наивысшей объясненной дисперсией.

5. Применить метод взаимной информации для определения важности признаков на основе их взаимной зависимости с целевой переменной.

6. Сравнить результаты различных методов выбора признаков и выбрать наиболее подходящий вариант.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import classification_report

# 1. Загрузка и предварительная обработка данных

# Предположим, что данные в одном столбце разделены запятыми
# Если данные разделены другим символом, замените ',' на нужный символ
data = pd.read_csv('/content/sample_data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', sep=',')  # Замените sep на нужный разделитель

data.head()

"""axis=1: Этот параметр указывает, что мы хотим удалить столбцы

inplace=True: Этот параметр указывает, что изменения должны быть применены непосредственно к оригинальному DataFrame data, а не возвращать новый DataFrame.
"""

# Удаление ненужных столбцов
data.drop([' Timestamp', 'Flow ID', ' Source IP', ' Destination IP'], axis=1, inplace=True)  # Удаляем столбцы, которые не нужны для анализа

data.head()

data.info()

# Преобразование категориальных признаков в числовые
data = pd.get_dummies(data, drop_first=True)  # Преобразуем категориальные признаки

"""fillna: Это метод библиотеки Pandas, который используется для заполнения пропущенных значений (NaN) в DataFrame


"""

# Заполнение пропущенных значений
data.fillna(data.median(), inplace=True)  # Заполняем пропуски медианой для числовых признаков

data.head()

data.info()

"""Корреляционная матрица помогает понять, как различные переменные в наборе данных связаны друг с другом. Это может быть полезно для выявления скрытых зависимостей и для предварительного анализа перед построением моделей машинного обучения.

correlation_matrix = data.corr(): Этот метод вычисляет корреляцию между всеми числовыми столбцами в DataFrame data. Результатом является матрица, где строки и столбцы представляют переменные, а значения в ячейках — коэффициенты корреляции. Коэффициенты варьируются от -1 до 1, где:

1 указывает на полную положительную корреляцию,

-1 указывает на полную отрицательную корреляцию,

0 указывает на отсутствие корреляции.


annot=True: Включает отображение значений коэффициентов корреляции на тепловой карте.

fmt=".2f": Указывает формат отображения значений (две цифры после запятой).
cmap='coolwarm': Устанавливает цветовую палитру для визуализации, где теплые цвета (красные) указывают на положительную корреляцию, а холодные (синие) — на отрицательную.


target_variable = ' Label_Infiltration': Указывает целевую переменную, с которой будет производиться анализ корреляции.

correlation_with_target = correlation_matrix[target_variable].abs().sort_values(ascending=False): Вычисляет абсолютные значения корреляции целевой переменной с другими признаками и сортирует их по убыванию. Это позволяет выявить признаки, которые наиболее сильно коррелируют с целевой переменной.


high_correlation_features = correlation_with_target[correlation_with_target > 0.1].index.tolist(): Отбирает признаки, у которых абсолютное значение корреляции больше 0.1, и сохраняет их названия в список. Это позволяет сосредоточиться на наиболее значимых признаках для дальнейшего анализа или построения модели.
"""

# 2. Рассчет матрицы корреляции
correlation_matrix = data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Матрица корреляции')
plt.show()

# Отбор признаков с наивысшей абсолютной корреляцией с целевой переменной (например, 'Label')
target_variable = ' Label_Infiltration'  # Замените на имя вашей целевой переменной
correlation_with_target = correlation_matrix[target_variable].abs().sort_values(ascending=False)
high_correlation_features = correlation_with_target[correlation_with_target > 0.1].index.tolist()  # Отбор признаков с корреляцией > 0.1

"""X = data[high_correlation_features].drop(target_variable, axis=1): Здесь мы создаем DataFrame X, который содержит все признаки, отобранные на основе их высокой корреляции с целевой переменной. Метод drop используется для удаления целевой переменной из X, чтобы оставить только признаки для обучения модели.

y = data[target_variable]: Здесь мы определяем целевую переменную y, которая будет использоваться для обучения модели. Она содержит значения, которые мы хотим предсказать.

model = RandomForestClassifier(): Создается экземпляр модели случайного леса, которая является мощным алгоритмом для задач классификации и регрессии. Он работает путем создания множества деревьев решений и объединения их предсказаний.

model.fit(X_train, y_train): Здесь модель обучается на обучающей выборке. Метод fit принимает признаки X_train и целевую переменную y_train и настраивает модель на основе этих данных.

Оценка важности признаков:
feature_importances = model.feature_importances_: После обучения модели мы можем получить важность каждого признака, используя атрибут feature_importances_. Это значение показывает, насколько каждый признак влияет на предсказания модели.

important_features = pd.Series(feature_importances, index=X.columns).sort_values(ascending=False): Здесь мы создаем серию Pandas, где индексами являются названия признаков, а значениями — их важность. Затем мы сортируем эту серию по убыванию важности, чтобы увидеть, какие признаки наиболее значимы для модели.
"""

# 3. Обучение модели машинного обучения
X = data[high_correlation_features].drop(target_variable, axis=1)  # Определяем признаки (X)
y = data[target_variable]  # Определяем целевую переменную (y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Разделяем данные на обучающую и тестовую выборки

# Обучение модели
model = RandomForestClassifier()  # Создаем экземпляр модели случайного леса
model.fit(X_train, y_train)  # Обучаем модель на обучающей выборке

# Оценка важности признаков
feature_importances = model.feature_importances_  # Получаем важность признаков
important_features = pd.Series(feature_importances, index=X.columns).sort_values(ascending=False)  # Сортируем признаки по важности

"""Объясненная дисперсия — это мера, которая показывает, какую долю общей дисперсии данных объясняет модель или набор факторов. В контексте статистики и машинного обучения объясненная дисперсия помогает оценить, насколько хорошо модель описывает вариации в целевой переменной.

Когда мы говорим о дисперсии в общем смысле, мы имеем в виду, насколько сильно значения отклоняются от среднего. Объясненная дисперсия, в свою очередь, фокусируется на том, как хорошо модель может объяснить эти отклонения.

Применение метода главных компонент (PCA) для уменьшения размерности данных, сохраняя при этом максимальную объясненную дисперсию.

Здесь мы создаем экземпляр класса PCA, указывая параметр n_components=0.95. Это означает, что мы хотим сохранить 95% объясненной дисперсии данных. PCA будет автоматически определять, сколько главных компонент необходимо для достижения этого уровня объясненной дисперсии.

fit: Вычисляет главные компоненты на основе входных данных X, определяя, как много информации (дисперсии) они объясняют.

transform: Применяет найденные главные компоненты к данным X, создавая новый набор данных X_pca, который имеет меньшую размерность, но сохраняет большую часть информации.


explained_variance = pca.explained_variance_ratio_: Этот атрибут возвращает массив, где каждый элемент представляет долю объясненной дисперсии для соответствующей главной компоненты. Это позволяет понять, сколько информации сохраняется в каждой из главных компонент.
"""

# 4. Применение метода главных компонент (PCA)
pca = PCA(n_components=0.95)  # Сохраняем 95% объясненной дисперсии
X_pca = pca.fit_transform(X)

# Объясненная дисперсия
explained_variance = pca.explained_variance_ratio_
print("Объясненная дисперсия главных компонент:", explained_variance)

"""Взаимная информация измеряет количество информации, которую один признак предоставляет о целевой переменной. Чем выше значение взаимной информации, тем больше информации о целевой переменной содержит данный признак. Это позволяет оценить, насколько каждый признак важен для предсказания целевой переменной."""

# 5. Метод взаимной информации
mi = mutual_info_classif(X, y)  # Вычисляем взаимную информацию между признаками и целевой переменной
mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)  # Создаем серию с взаимной информацией и сортируем по убыванию

# 6. Сравнение результатов различных методов выбора признаков
print("Важность признаков по Random Forest:")
print(important_features.head(10))  # Выводим 10 наиболее важных признаков по Random Forest

print("\nВзаимная информация:")
print(mi_series.head(10))  # Выводим 10 признаков с наибольшей взаимной информацией

# Сравнение результатов
plt.figure(figsize=(12, 6))  # Устанавливаем размер графика
important_features.head(10).plot(kind='bar', color='blue', alpha=0.7, label='Важность признаков')  # Столбчатая диаграмма для важности признаков
mi_series.head(10).plot(kind='bar', color='orange', alpha=0.7, label='Взаимная информация')  # Столбчатая диаграмма для взаимной информации
plt.title('Сравнение важности признаков и взаимной информации')  # Заголовок графика
plt.xlabel('Признаки')  # Подпись оси X
plt.ylabel('Оценка')  # Подпись оси Y
plt.legend()  # Добавляем легенду
plt.show()  # Отображаем график